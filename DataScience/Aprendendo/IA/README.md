# üß† Redes Neurais

## üìå O que s√£o Redes Neurais?
Redes neurais s√£o **modelos computacionais** inspirados no funcionamento do **c√©rebro humano**. Elas s√£o compostas por **neur√¥nios artificiais** (perceptrons), que processam informa√ß√µes e aprendem com os dados.

### **Componentes B√°sicos**
1. **Neur√¥nios (N√≥s)**: Unidades que processam entradas e produzem sa√≠das.
2. **Conex√µes (Sinapses)**: Liga√ß√µes entre neur√¥nios com pesos que influenciam o aprendizado.
3. **Camadas**:
   - **Entrada**: Recebe os dados.
   - **Ocultas**: Processam informa√ß√µes.
   - **Sa√≠da**: Produz o resultado final.
4. **Fun√ß√£o de Ativa√ß√£o**: Introduz **n√£o-linearidade** para permitir que a rede aprenda padr√µes complexos.
5. **Pesos e Bias**: Ajust√°veis durante o treinamento.

---

# üîπ Tipos de Redes Neurais

## **1Ô∏è‚É£ Perceptron Simples (Single-Layer Perceptron)**
O **Perceptron Simples** resolve **problemas de classifica√ß√£o bin√°ria** (ex: Sim/N√£o, 0/1).

### **Funcionamento**
1. **Recebe entradas** (`x1, x2, ..., xn`).
2. **Multiplica por pesos** (`w1, w2, ..., wn`).
3. **Soma ponderada**:  
   \[
   z = (w1 \cdot x1 + w2 \cdot x2 + ... + wn \cdot xn) + bias
   \]
4. **Aplica uma fun√ß√£o de ativa√ß√£o** para determinar a sa√≠da (`0` ou `1`).

### **‚ö†Ô∏è Limita√ß√£o**
- S√≥ resolve problemas **linearmente separ√°veis** (n√£o consegue resolver **XOR**).

---

## **2Ô∏è‚É£ Redes Neurais com M√∫ltiplas Camadas (MLP)**
As **Redes Neurais Multicamadas (MLPs)** possuem **camadas ocultas**, permitindo aprender padr√µes **n√£o-lineares**.

### **Funcionamento**
1. **Camada de Entrada** ‚Üí Recebe os dados.
2. **Camadas Ocultas** ‚Üí Processam informa√ß√µes.
3. **Camada de Sa√≠da** ‚Üí Produz o resultado final.

### **Backpropagation (Aprendizado)**
1. **Forward Propagation**: Calcula a sa√≠da.
2. **C√°lculo do Erro**: Mede a diferen√ßa entre a sa√≠da prevista e a real.
3. **Backpropagation**: Ajusta os pesos usando **Gradiente Descendente**.
4. **Repeti√ß√£o**: At√© minimizar o erro.

### **Fun√ß√µes de Ativa√ß√£o**
| Fun√ß√£o | F√≥rmula | Vantagens | Desvantagens |
|--------|---------|-----------|-------------|
| **Sigmoid** | \( f(x) = \frac{1}{1 + e^{-x}} \) | Boa para probabilidades | Sofre com **vanishing gradient** |
| **Tanh** | \( f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \) | Melhor que sigmoid | Tamb√©m sofre **vanishing gradient** |
| **ReLU** | \( f(x) = \max(0, x) \) | R√°pida e eficiente | Pode sofrer de **neur√¥nios mortos** |
| **Softmax** | \( f(x_i) = \frac{e^{x_i}}{\sum e^{x_j}} \) | Para **classifica√ß√£o multiclasse** | - |

---

## **3Ô∏è‚É£ Redes Neurais Profundas (DNNs)**
As **Redes Neurais Profundas (DNNs)** possuem **m√∫ltiplas camadas ocultas**, permitindo aprender representa√ß√µes **abstratas**.

### **Vantagens das DNNs**
‚úÖ Capturam padr√µes **altamente n√£o-lineares**.  
‚úÖ Usadas em **reconhecimento de imagem, NLP, previs√£o de s√©ries temporais**.  

### **Desafios das DNNs**
‚ö†Ô∏è **Overfitting** ‚Üí Aprende muito bem o treino, mas falha nos dados reais.  
‚ö†Ô∏è **Exploding/Vanishing Gradient** ‚Üí Os gradientes ficam grandes ou pequenos demais.  
‚ö†Ô∏è **Tempo de Treinamento** ‚Üí Redes grandes precisam de mais dados e computa√ß√£o.

### **T√©cnicas para Melhorar DNNs**
‚úÖ **Dropout** ‚Üí Desativa neur√¥nios aleatoriamente para evitar overfitting.  
‚úÖ **Batch Normalization** ‚Üí Normaliza ativa√ß√µes para acelerar o aprendizado.  
‚úÖ **Inicializa√ß√£o de Pesos (Xavier, He)** ‚Üí Evita problemas com gradientes.  

---

# üöÄ **T√©cnicas Avan√ßadas**
| T√©cnica | Descri√ß√£o |
|---------|----------|
| **Regulariza√ß√£o L2** | Penaliza pesos altos para evitar overfitting. |
| **Momentum** | Acelera o gradiente descendente, evitando m√≠nimos locais. |
| **Adam Optimizer** | Um dos otimizadores mais eficientes. |
| **CNNs** | Especializadas para **imagens**. |
| **RNNs** | Usadas para **textos e s√©ries temporais**. |

---

# **üìå Resumo**
‚úÖ **Perceptron Simples** ‚Üí Apenas problemas **linearmente separ√°veis**.  
‚úÖ **MLPs (Redes com uma camada oculta)** ‚Üí Aprendem padr√µes **n√£o-lineares**.  
‚úÖ **DNNs (Redes Profundas)** ‚Üí Aprendem representa√ß√µes **abstratas** e **complexas**.  
‚úÖ **T√©cnicas como Dropout e Adam** melhoram o desempenho.  

---