import numpy as np

import numpy as np

class NeuralNetwork:
    def __init__(self, learning_rate=0.01, epochs=10000):
        # Pesos e bias para a camada oculta (2 neur√¥nios)
        self.weights_hidden = np.random.rand(2, 4)  # 2 entradas, 2 neur√¥nios na camada oculta
        self.bias_hidden = np.random.rand(4)  

        # Pesos e bias para a camada de sa√≠da (1 neur√¥nio)
        self.weights_output = np.random.rand(4, 1)  # Corrigido para matriz (2,1)
        self.bias_output = np.random.rand(1)  

        # Par√¢metros de treinamento
        self.learning_rate = learning_rate  
        self.epochs = epochs  

    # fun√ß√µes de ativa√ß√£o
    # SIGMOID
    def sigmoid(self, x):
        """Fun√ß√£o de ativa√ß√£o sigm√≥ide."""
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        """Derivada da sigm√≥ide para ajustar os pesos."""
        return x * (1 - x)

    # RELU
    def relu(self, x):
        return np.maximum(0, x)

    def relu_derivative(self, x):
        return np.where(x > 0, 1, 0)
    #üìå Vantagens:
    #‚úÖ Resolve o problema do vanishing gradient da sigm√≥ide.
    #‚úÖ √â mais eficiente para redes mais profundas.

    # TANH
    def tanh(self, x):
        return np.tanh(x)

    def tanh_derivative(self, x):
        return 1 - np.tanh(x) ** 2
    # üìå Vantagens:
    # ‚úÖ Retorna valores entre -1 e 1, sendo √∫til para normaliza√ß√£o.
    # ‚úÖ Pode aprender mais r√°pido que sigm√≥ide.

    def predict(self, X):
        """Faz previs√µes para uma entrada X e arredonda para 0 ou 1."""
        hidden_input = np.dot(X, self.weights_hidden) + self.bias_hidden  
        hidden_output = self.tanh(hidden_input)  

        final_input = np.dot(hidden_output, self.weights_output) + self.bias_output  
        final_output = self.tanh(final_input)  

        return np.round(final_output)  # Arredonda para 0 ou 1

    def train(self, X, y):
        """Treina a rede neural."""
        for epoch in range(self.epochs):
            # Forward Pass
            hidden_input = np.dot(X, self.weights_hidden) + self.bias_hidden  
            hidden_output = self.tanh(hidden_input)  

            final_input = np.dot(hidden_output, self.weights_output) + self.bias_output  
            final_output = self.tanh(final_input)  

            # Calcular erro
            error = y - final_output  

            # Backpropagation (Ajuste dos pesos)
            d_output = error * self.tanh_derivative(final_output)  
            d_hidden = np.dot(d_output, self.weights_output.T) * self.tanh_derivative(hidden_output)

            lambda_reg = 0.001  # Define a intensidade da regulariza√ß√£o (IDEAL -> learning_rate / 10)

            # Atualiza√ß√£o dos pesos e bias
            self.weights_output += self.learning_rate * np.dot(hidden_output.T, d_output)
            self.bias_output += self.learning_rate * np.sum(d_output, axis=0)
            self.weights_output -= lambda_reg * self.weights_output  # Regulariza√ß√£o

            self.weights_hidden += self.learning_rate * np.dot(X.T, d_hidden)
            self.bias_hidden += self.learning_rate * np.sum(d_hidden, axis=0)
            self.weights_hidden -= lambda_reg * self.weights_hidden  # Regulariza√ß√£o

            # Exibir erro a cada 1000 √©pocas
            if epoch % 1000 == 0:
                print(f"√âpoca {epoch}, Erro m√©dio: {np.mean(np.abs(error))}")

# Dados de entrada (X1, X2) e sa√≠da esperada (XOR)
X = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])

y = np.array([[0], [1], [1], [0]])  # Sa√≠das esperadas para XOR

# Criar e treinar a rede neural
nn = NeuralNetwork(learning_rate=0.1, epochs=10000)
nn.train(X, y)

# Testar a rede neural ap√≥s o treinamento
print("\nTestes XOR:")
for inputs in X:
    print(f"Entrada: {inputs}, Previs√£o: {nn.predict(inputs)}")
